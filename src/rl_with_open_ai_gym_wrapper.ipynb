{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through the implementation of the wrapper code to train Pokemon teams for battle using keras-rl2 agents. \n",
    "\n",
    "**What is wrapper code?**\n",
    "Well, wrapper code is used here to coordinate between the keras-rl2 library, which contains implementations of several key reinforcement learning algorithms, and the Pokemon Showdown environment. It enables us to train agents which play in the Pokemon Showdown environment using agents from keras-rl2.\n",
    "\n",
    "The first thing we do is to import nest_asyncio and to apply it. You need not worry too much about how this library works, except to know that Jupyter notebooks execute an event loop, but the code we are running also utilizes an event loop in the backend, so we require this library to allow two event loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the type of agent we would like to use in this notebook in the variable 'model_type', so that we can check this variable later in the notebook before declaring the relevant model (DDQN vs SARSA vs CEM). You must restart the kernel every time you want to begin training again.\n",
    "\n",
    "The frozen_model_name should be declared based on the frozen model that you would like to use as an opponent. This can be different from your training agent. For example, my opponent may be a dqn whilst training a CEM agent. If you do not yet have a frozen model opponent, you do not need to declare the FROZEN_MODEL_NAME. Just switch FROZEN_MODEL_PRESENT to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"dqn\"\n",
    "FROZEN_MODEL_PRESENT=True\n",
    "FROZEN_MODEL_NAME= 'CEM'\n",
    "NB_STEPS_WARMUP = 1000\n",
    "\n",
    "\n",
    "#options = cem, sarsa, dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity, lets walk through each of the imports in the cell below. \n",
    "\n",
    "# General Libraries\n",
    "*numpy*: this is a common library used to manipulate arrays & perform array-wise operations etc.\n",
    "*tensorflow*: this is the library we use to train deep neural networks. We will perform deep reinforcement learning (not just reinforcement learning), so we require this library.\n",
    "*pandas*: this is a common library used to manipulate tables & dataframes.\n",
    "\n",
    "# poke-env imports\n",
    " These imports help us to correspond with the Pokemon Showdown environment. \n",
    " \n",
    "*PlayerConfiguration*: a class that allows us to store simple attributes such as player username and password.\n",
    "*LocalhostServerConfiguration*: specifies which localhost we are using to specify our Pokemon-Showdown server. We can also use the main Pokemon-Showdown server (remote) but this would be increasing the burden on the main server which many people are using, so this is highly discouraged\n",
    "\n",
    "There exists a base class called Player, from which Gen7EnvSinglePlayer, RandomPlayer and FrozenRLPlayer each inherit.\n",
    "\n",
    "*Gen7EnvSinglePlayer*: this is the class that will be the parent of our reinforcement learning agent class(later declared as SimpleRLPlayer that we will be using for training. It enables us to start battles on the Pokemon Showdown environment, update our neural network etc.\n",
    "\n",
    "*RandomPlayer*: this is the class that will define players that take completely random moves at each step. It also functions as the parent of the MaxDamagePlayer class. The MaxDamagePlayer will take the move which causes **maximum** damage based on the base power of the move. This is not necessarily the best move in the long-term. Both RandomPlayer and MaxDamagePlayer are used as opponents to our SimpleRLPlayer during training.\n",
    "\n",
    "*FrozenRLPlayer*: Similar to RandomPlayer and MaxDamagePlayer, this player is used as an opponent to the SimpleRLPlayer during training. The FrozenRLPlayer is initialized using a pre-trained RL agent from a previous iteration of training the SimpleRLPlayer. Using this FrozenRLPlayer we can include some self-play in our implementation.\n",
    "\n",
    "# keras-rl imports\n",
    "These imports help us to correspond with keras-rl2 agents.\n",
    "\n",
    "*CEMAgent*, *DQNAgent*, *SARSAAgent*: These are self-explanatorily imports of each of the respective RL agents that we utilize. \n",
    "\n",
    "*rl.policy imports*: These help us to import different policies for our agents to follow. For example, an epsilon greedy policy means that our agent chooses the move that has the highest value, but chooses a random move with probability epsilon, to aid in exploration (vs the pure exploitation that would be undertaken by a greedy policy). The linear annealment allows us to decay epsilon (erring towards exploitation later in training).\n",
    "\n",
    "*rl.memory imports*: These help us to import different types of memory. For example, ddqn and sarsa use sequential memory, whilst cem uses episode parameter memory.\n",
    "\n",
    "\n",
    "# tensorflow imports\n",
    "The *tensorflow.keras* imports are self-explanatory for anyone who has used tensorflow. We are importing the different types of layers, models and optimizers. Here is a nice intro to keras, for those unfamiliar with it: https://towardsdatascience.com/introduction-to-deep-learning-with-keras-17c09e4f0eb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.player.env_player import Gen7EnvSinglePlayer\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.frozen_rl_player import FrozenRLPlayer\n",
    "from poke_env.server_configuration import LocalhostServerConfiguration\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory, EpisodeParameterMemory\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Our Players\n",
    "\n",
    "We pre-imported our Gen7EnvSinglePlayer, RandomPlayer and FrozenRLPlayer (I would strongly encourage you to look into both those files to see how we define these classes). However, below, we define the SimpleRLPlayer, which is the agent that we will train, and the MaxDamagePlayer, which is one of our three types of opponents, as explained earlier.\n",
    "\n",
    "*SimpleRLPlayer*\n",
    "We can see that the player has two methods defined here i.e. *embed_battle* and *compute_reward* respectively. *embed_battle* is used to embed the current state of the battle between two pokemon teams. *compute_reward* is used to compute the current reward. We require both of these things in order to select the next action, as well as to update our deep RL model weights. You will find it interesting to note that the FrozenRLPlayer has very similar methods to these two, and can be thought of as a simpler version of SimpleRLPlayer that has fixed weights, and cannot be updated or start battles. FrozenRLPlayer inherits straight from Player, unlike SimpleRLPlayer which is more powerful and inherits from Gen7EnvSinglePlayer.\n",
    "\n",
    "*MaxDamagePlayer*\n",
    "As we can see, the next move is decided purely on the basis of the base power of each of the moves available to the pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our RL player\n",
    "# It needs a state embedder and a reward computer, hence these two methods\n",
    "class SimpleRLPlayer(Gen7EnvSinglePlayer):\n",
    "    def embed_battle(self, battle):\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have not fainted in each team\n",
    "        remaining_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "        remaining_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        return np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [remaining_mon_team, remaining_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            battle, fainted_value=2, hp_value=1, victory_value=30\n",
    "        )\n",
    "\n",
    "\n",
    "class MaxDamagePlayer(RandomPlayer):\n",
    "    def choose_move(self, battle):\n",
    "        # If the player can attack, it will\n",
    "        if battle.available_moves:\n",
    "            # Finds the best move among available ones\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "\n",
    "        # If no attack is available, a random switch will be made\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to train!\n",
    "\n",
    "Finally, we are ready to think about training our agents. We quickly define two functions, one for training and one for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRAINING_STEPS = 10000\n",
    "NB_EVALUATION_EPISODES = 100\n",
    "\n",
    "# variable for naming .csv files.\n",
    "# Change this according to whether the training process was carried out against a random player or a max damage player\n",
    "TRAINING_OPPONENT = 'RandomPlayer'\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# This is the function that will be used to train the agent\n",
    "def agent_training(player, agent, nb_steps, filename):\n",
    "    model = agent.fit(player, nb_steps=nb_steps)\n",
    "    # save model history to csv\n",
    "    save_file = f\"{filename}_trainlog_{nb_steps}eps.csv\"\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Saving model history as {save_file}\")\n",
    "    print(\"===============================================\")\n",
    "    pd.DataFrame(model.history).to_csv(save_file)\n",
    "    player.complete_current_battle()\n",
    "\n",
    "\n",
    "def agent_evaluation(player, agent, nb_episodes, filename):\n",
    "    # Reset battle statistics\n",
    "    player.reset_battles()\n",
    "    model = agent.test(player, nb_episodes=nb_episodes, visualize=False, verbose=False)\n",
    "\n",
    "    # save model history to csv\n",
    "    save_file = f\"{filename}_testlog_{nb_episodes}eps.csv\"\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Saving model history as {save_file}\")\n",
    "    print(\"===============================================\")\n",
    "    pd.DataFrame(model.history).to_csv(save_file)\n",
    "    \n",
    "    print(\n",
    "          \"CEM Evaluation: %d victories out of %d episodes\"\n",
    "          % (player.n_won_battles, nb_episodes)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we are just predefining the right frozen model to use for the FrozenRLPlayer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dimension\n",
    "n_action = 18\n",
    "########################## Trained RL Model variables ##########################\n",
    "\n",
    "if FROZEN_MODEL_PRESENT:\n",
    "    if FROZEN_MODEL_NAME == 'CEM':\n",
    "        ### CHANGE THE LOAD MODEL DIRECTORY ACCORDING TO LOCAL SETUP ###\n",
    "        loaded_model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/modelmax_20000')\n",
    "        memory = EpisodeParameterMemory(limit=10000, window_length=1)\n",
    "        trained_agent = CEMAgent(model=loaded_model, nb_actions=18, memory=memory,\n",
    "                       batch_size=50, nb_steps_warmup=NB_STEPS_WARMUP, train_interval=50, elite_frac=0.05, noise_ampl=0)\n",
    "    elif FROZEN_MODEL_NAME == 'DQN':\n",
    "        ### CHANGE THE LOAD MODEL DIRECTORY ACCORDING TO LOCAL SETUP ###\n",
    "        loaded_model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/src/DQNAgent/modeldqn_20000')\n",
    "        memory = SequentialMemory(limit=10000, window_length=1)\n",
    "        #Simple epsilon greedy policy\n",
    "        policy = LinearAnnealedPolicy(\n",
    "            EpsGreedyQPolicy(),\n",
    "            attr=\"eps\",\n",
    "            value_max=1.0,\n",
    "            value_min=0.05,\n",
    "            value_test=0,\n",
    "            nb_steps=10000,\n",
    "        )\n",
    "\n",
    "        # load saved model into DQNAgent class\n",
    "        trained_agent = DQNAgent(\n",
    "                model=loaded_model,\n",
    "                nb_actions=18,\n",
    "                policy=policy,\n",
    "                memory=memory,\n",
    "                nb_steps_warmup=NB_STEPS_WARMUP,\n",
    "                gamma=0.5,\n",
    "                target_model_update=1,\n",
    "                delta_clip=0.01,\n",
    "                enable_double_dqn=True,\n",
    "            )\n",
    "\n",
    "    elif FROZEN_MODEL_NAME == 'SARSA':\n",
    "        ### CHANGE THE LOAD MODEL DIRECTORY ACCORDING TO LOCAL SETUP ###\n",
    "        loaded_model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/src/SARSAAgent/model_20000')\n",
    "        memory = SequentialMemory(limit=10000, window_length=1)\n",
    "        #Simple epsilon greedy policy\n",
    "        policy = LinearAnnealedPolicy(\n",
    "            EpsGreedyQPolicy(),\n",
    "            attr=\"eps\",\n",
    "            value_max=1.0,\n",
    "            value_min=0.05,\n",
    "            value_test=0,\n",
    "            nb_steps=10000,\n",
    "        )\n",
    "        trained_agent = SARSAAgent(model=loaded_model, nb_actions=n_action, nb_steps_warmup=1000, policy=policy)\n",
    "else:\n",
    "    print(\"you will not be able to use third(frozen) opponent, because you have not enabled this\")\n",
    "\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are performing **deep** reinforcement learning, so we first need to define a keras network for this training. We define the network slightly differently, depending on whether we are using CEM, DQN or SARSA. In the cell below, we first instantiate the four sorts of players that we use. Then, we define the neural network structures. For DQN & SARSA, we use identical structures and a linear activation, whilst we use a softmax activation for CEM. This makes sense because CEM automatically predicts the policy, whilst DQN and SARSA predict the q-values which are then converted into a policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 122s 12ms/step - reward: 0.3460\n",
      "done, took 121.656 seconds\n",
      "===============================================\n",
      "Saving model history as RandomPlayerNotebook_trainlog_10000eps.csv\n",
      "===============================================\n",
      "INFO:tensorflow:Assets written to: model_notebook_10000/assets\n",
      "Results against random player:\n",
      "===============================================\n",
      "Saving model history as (RandomPlayer_10000)RandomPlayerNotebook_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 93 victories out of 100 episodes\n",
      "\n",
      "Results against max player:\n",
      "===============================================\n",
      "Saving model history as (RandomPlayer_10000)MaxPlayerNotebook_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 61 victories out of 100 episodes\n",
      "\n",
      "Results against frozen rl player:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_player = SimpleRLPlayer(\n",
    "        player_configuration=PlayerConfiguration(\"satunicarina\", None),\n",
    "        battle_format=\"gen7randombattle\",\n",
    "        server_configuration=LocalhostServerConfiguration,\n",
    "    )\n",
    "\n",
    "    opponent = RandomPlayer(\n",
    "        player_configuration=PlayerConfiguration(\"duanicarina\", None),\n",
    "        battle_format=\"gen7randombattle\",\n",
    "        server_configuration=LocalhostServerConfiguration,\n",
    "    )\n",
    "\n",
    "    second_opponent = MaxDamagePlayer(\n",
    "        player_configuration=PlayerConfiguration(\"tiganicarina\", None),\n",
    "        battle_format=\"gen7randombattle\",\n",
    "        server_configuration=LocalhostServerConfiguration,\n",
    "    )\n",
    "    if FROZEN_MODEL_PRESENT:\n",
    "        third_opponent = FrozenRLPlayer(\n",
    "            player_configuration=PlayerConfiguration(\"empatnicarina\", None), \n",
    "            battle_format=\"gen7randombattle\", \n",
    "            server_configuration=LocalhostServerConfiguration,         \n",
    "            trained_rl_model=trained_agent,\n",
    "            model_name = FROZEN_MODEL_NAME,)\n",
    "\n",
    "    \n",
    "    if model_type=='cem':\n",
    "        # Output dimension\n",
    "        memory = EpisodeParameterMemory(limit=10000, window_length=1)\n",
    "        # deep network\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1, 10)))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_action))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # Ssimple epsilon greedy\n",
    "        policy = LinearAnnealedPolicy(\n",
    "            EpsGreedyQPolicy(),\n",
    "            attr=\"eps\",\n",
    "            value_max=1.0,\n",
    "            value_min=0.05,\n",
    "            value_test=0,\n",
    "            nb_steps=10000,\n",
    "        )\n",
    "#         #only uncomment below line if you want to continue training an old model\n",
    "#         model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/modelpostmax2preserve_20000')\n",
    "\n",
    "\n",
    "        # Defining our agent\n",
    "        agent = CEMAgent(model=model, nb_actions=n_action, memory=memory,\n",
    "                       batch_size=50, nb_steps_warmup=1000, train_interval=50, elite_frac=0.05, noise_ampl=4)\n",
    "\n",
    "\n",
    "        agent.compile()\n",
    "    \n",
    "    elif model_type=='dqn' or model_type=='sarsa':\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, activation=\"elu\", input_shape=(1, 10)))\n",
    "\n",
    "        # Our embedding have shape (1, 10), which affects our hidden layer\n",
    "        # dimension and output dimension\n",
    "        # Flattening resolve potential issues that would arise otherwise\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation=\"elu\"))\n",
    "        model.add(Dense(n_action, activation=\"linear\"))\n",
    "\n",
    "        memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "        # Simple epsilon greedy\n",
    "        policy = LinearAnnealedPolicy(\n",
    "            EpsGreedyQPolicy(),\n",
    "            attr=\"eps\",\n",
    "            value_max=1.0,\n",
    "            value_min=0.05,\n",
    "            value_test=0,\n",
    "            nb_steps=10000,\n",
    "        )\n",
    "#         #only uncomment below line if you want to continue training an old model\n",
    "#         model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/modelpostmax2preserve_20000')\n",
    "\n",
    "        # Defining our DQN\n",
    "        if model_type=='dqn':\n",
    "            agent = DQNAgent(\n",
    "                model=model,\n",
    "                nb_actions=18,\n",
    "                policy=policy,\n",
    "                memory=memory,\n",
    "                nb_steps_warmup=1000,\n",
    "                gamma=0.5,\n",
    "                target_model_update=1,\n",
    "                delta_clip=0.01,\n",
    "                enable_double_dqn=True,\n",
    "            )\n",
    "        elif model_type=='sarsa':\n",
    "            agent = SARSAAgent(model=model, nb_actions=n_action, nb_steps_warmup=1000, policy=policy)\n",
    "\n",
    "        agent.compile(Adam(lr=0.00025), metrics=[\"mae\"])\n",
    "        \n",
    "\n",
    "\n",
    "    # Training\n",
    "    env_player.play_against(\n",
    "        env_algorithm=agent_training,\n",
    "        opponent=third_opponent,\n",
    "                            env_algorithm_kwargs={\"agent\": agent, \"nb_steps\": NB_TRAINING_STEPS, \"filename\": TRAINING_OPPONENT+'Notebook'},\n",
    "    )\n",
    "    model.save(\"model_notebook_%d\" % NB_TRAINING_STEPS)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"Results against random player:\")\n",
    "    env_player.play_against(\n",
    "        env_algorithm=agent_evaluation,\n",
    "        opponent=opponent,\n",
    "        env_algorithm_kwargs={\"agent\": agent, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'({TRAINING_OPPONENT}_{NB_TRAINING_STEPS})RandomPlayerNotebook'},\n",
    "    )\n",
    "\n",
    "    print(\"\\nResults against max player:\")\n",
    "    env_player.play_against(\n",
    "        env_algorithm=agent_evaluation,\n",
    "        opponent=second_opponent,\n",
    "        env_algorithm_kwargs={\"agent\": agent, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'({TRAINING_OPPONENT}_{NB_TRAINING_STEPS})MaxPlayerNotebook'},\n",
    "    )\n",
    "\n",
    "    if FROZEN_MODEL_PRESENT:\n",
    "        print(\"\\nResults against frozen rl player:\")\n",
    "        env_player.play_against(\n",
    "                                env_algorithm=agent_evaluation,\n",
    "                                opponent=third_opponent,\n",
    "                                env_algorithm_kwargs={\"agent\": agent, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'({TRAINING_OPPONENT}_{NB_TRAINING_STEPS})MaxPlayerNotebook'},\n",
    "                                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krl-env",
   "language": "python",
   "name": "krl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
