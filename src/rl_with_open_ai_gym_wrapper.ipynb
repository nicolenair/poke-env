{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dependencies using a virtual environment\n",
    "When you clone this repository, you will have access to a 'requirements-clean.txt' file. In your terminal, set up a virtual-env and install all the requirements using 'pip install requirements-clean.txt' as per usual. If you are unfamiliar with this process, read [this](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) article.\n",
    "\n",
    "To add this virtual-env in this Jupyter Notebook, use the instructions linked [here](https://janakiev.com/blog/jupyter-virtual-envs/), specifically the section entitled 'Add Virtual Environment to Jupyter Notebook.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this tutorial\n",
    "This notebook walks you through the implementation of the wrapper code to train Pokemon teams for battle using keras-rl2 agents. \n",
    "\n",
    "**What is wrapper code?**\n",
    "Well, wrapper code is used here to coordinate between the keras-rl2 library, which contains implementations of several key reinforcement learning algorithms, and the Pokemon Showdown environment. It enables us to train agents which play in the Pokemon Showdown environment using agents from keras-rl2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we begin!\n",
    "The first thing we do is to import nest_asyncio and to apply it. You need not worry too much about how this library works, except to know that Jupyter notebooks execute an event loop, but the code we are running also utilizes an event loop in the backend, so we require this library to allow two event loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the type of agent we would like to use in this notebook in the variable 'model_type', so that we can check this variable later in the notebook before declaring the relevant model (DQN vs SARSA vs CEM). You must restart the kernel every time you want to begin training again.\n",
    "\n",
    "The `FROZEN_MODEL_NAME` should be declared based on the frozen model that you would like to use as an opponent. This can be different from your training agent. For example, my opponent may be a dqn whilst training a CEM agent. If you do not yet have a frozen model opponent, you do not need to declare the `FROZEN_MODEL_NAME`. Just switch `FROZEN_MODEL_PRESENT` to **`False`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"DQN\" # options = DQN, CEM, SARSA\n",
    "FROZEN_MODEL_PRESENT = True\n",
    "FROZEN_MODEL_NAME = \"CEM\" # options = DQN, CEM, SARSA\n",
    "NB_STEPS_WARMUP = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity, lets walk through each of the imports in the cell below. \n",
    "\n",
    "# General Libraries\n",
    "*numpy*: this is a common library used to manipulate arrays & perform array-wise operations etc.\n",
    "*tensorflow*: this is the library we use to train deep neural networks. We will perform deep reinforcement learning (not just reinforcement learning), so we require this library.\n",
    "*pandas*: this is a common library used to manipulate tables & dataframes.\n",
    "\n",
    "# poke-env imports\n",
    " These imports help us to correspond with the Pokemon Showdown environment. \n",
    " \n",
    "*PlayerConfiguration*: a class that allows us to store simple attributes such as player username and password.\n",
    "*LocalhostServerConfiguration*: specifies which localhost we are using to specify our Pokemon-Showdown server. We can also use the main Pokemon-Showdown server (remote) but this would be increasing the burden on the main server which many people are using, so this is highly discouraged\n",
    "\n",
    "There exists a base class called Player, from which Gen7EnvSinglePlayer, RandomPlayer and FrozenRLPlayer each inherit.\n",
    "\n",
    "*Gen7EnvSinglePlayer*: this is the class that will be the parent of our reinforcement learning agent class(later declared as SimpleRLPlayer that we will be using for training. It enables us to start battles on the Pokemon Showdown environment, update our neural network etc.\n",
    "\n",
    "*RandomPlayer*: this is the class that will define players that take completely random moves at each step. It also functions as the parent of the MaxDamagePlayer class. The MaxDamagePlayer will take the move which causes **maximum** damage based on the base power of the move. This is not necessarily the best move in the long-term. Both RandomPlayer and MaxDamagePlayer are used as opponents to our SimpleRLPlayer during training.\n",
    "\n",
    "*FrozenRLPlayer*: Similar to RandomPlayer and MaxDamagePlayer, this player is used as an opponent to the SimpleRLPlayer during training. The FrozenRLPlayer is initialized using a pre-trained RL agent from a previous iteration of training the SimpleRLPlayer. Using this FrozenRLPlayer we can include some self-play in our implementation.\n",
    "\n",
    "# keras-rl imports\n",
    "These imports help us to correspond with keras-rl2 agents.\n",
    "\n",
    "*CEMAgent*, *DQNAgent*, *SARSAAgent*: These are self-explanatorily imports of each of the respective RL agents that we utilize. \n",
    "\n",
    "*rl.policy imports*: These help us to import different policies for our agents to follow. For example, an epsilon greedy policy means that our agent chooses the move that has the highest value, but chooses a random move with probability epsilon, to aid in exploration (vs the pure exploitation that would be undertaken by a greedy policy). The linear annealment allows us to decay epsilon (erring towards exploitation later in training).\n",
    "\n",
    "*rl.memory imports*: These help us to import different types of memory. For example, ddqn and sarsa use sequential memory, whilst cem uses episode parameter memory.\n",
    "\n",
    "\n",
    "# tensorflow imports\n",
    "The *tensorflow.keras* imports are self-explanatory for anyone who has used tensorflow. We are importing the different types of layers, models and optimizers. [Here](https://towardsdatascience.com/introduction-to-deep-learning-with-keras-17c09e4f0eb2) is a nice intro to keras, for those unfamiliar with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.player.env_player import Gen7EnvSinglePlayer\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.frozen_rl_player import FrozenRLPlayer\n",
    "from poke_env.server_configuration import LocalhostServerConfiguration\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory, EpisodeParameterMemory\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Our Players\n",
    "\n",
    "We pre-imported our Gen7EnvSinglePlayer, RandomPlayer and FrozenRLPlayer (I would strongly encourage you to look into both those files to see how we define these classes). However, below, we define the SimpleRLPlayer, which is the agent that we will train, and the MaxDamagePlayer, which is one of our three types of opponents, as explained earlier.\n",
    "\n",
    "*SimpleRLPlayer*\n",
    "We can see that the player has two methods defined here i.e. *embed_battle* and *compute_reward* respectively. *embed_battle* is used to embed the current state of the battle between two pokemon teams. *compute_reward* is used to compute the current reward. We require both of these things in order to select the next action, as well as to update our deep RL model weights. You will find it interesting to note that the FrozenRLPlayer has very similar methods to these two, and can be thought of as a simpler version of SimpleRLPlayer that has fixed weights, and cannot be updated or start battles. FrozenRLPlayer inherits straight from Player, unlike SimpleRLPlayer which is more powerful and inherits from Gen7EnvSinglePlayer.\n",
    "\n",
    "*MaxDamagePlayer*\n",
    "As we can see, the next move is decided purely on the basis of the base power of each of the moves available to the pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleRLPlayer(Gen7EnvSinglePlayer):\n",
    "    '''\n",
    "    We define our RL player\n",
    "    It needs a state embedder and a reward computer, hence these two methods\n",
    "    '''\n",
    "    \n",
    "    def embed_battle(self, battle):\n",
    "        \n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have not fainted in each team\n",
    "        remaining_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "        remaining_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        return np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [remaining_mon_team, remaining_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    \n",
    "    def compute_reward(self, battle) -> float:\n",
    "        \n",
    "        return self.reward_computing_helper(\n",
    "            battle, fainted_value=2, hp_value=1, victory_value=30\n",
    "        )\n",
    "\n",
    "\n",
    "class MaxDamagePlayer(RandomPlayer):\n",
    "    '''\n",
    "    A player that chooses the move that deals max damage each time.\n",
    "    '''\n",
    "    def choose_move(self, battle):\n",
    "        \n",
    "        # If the player can attack, it will\n",
    "        if battle.available_moves:\n",
    "            \n",
    "            # Finds the best move among available ones\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "\n",
    "        # If no attack is available, a random switch will be made\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Training and Testing\n",
    "\n",
    "The functions below are used for training and testing the RL agents. The train and test logs are saved as `.csv` files for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRAINING_STEPS = 20000\n",
    "NB_EVALUATION_EPISODES = 100\n",
    "\n",
    "# variable for naming .csv files.\n",
    "# Change this according to the type of opponent, e.g. MaxPlayer, RandomPlayer, Pretrained RL Player\n",
    "TRAINING_OPPONENT = 'FrozenRLPlayerDQN'\n",
    "\n",
    "# set random seeds\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Training and testing functions\n",
    "def agent_training(player, agent, nb_steps, filename):\n",
    "    '''\n",
    "    Function for training agent and saving the model history.\n",
    "    '''\n",
    "    model = agent.fit(player, nb_steps=nb_steps)\n",
    "    \n",
    "    # save model history to csv\n",
    "    save_file = f\"{filename}_trainlog_{nb_steps}eps.csv\"\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Saving model history as {save_file}\")\n",
    "    print(\"===============================================\")\n",
    "    pd.DataFrame(model.history).to_csv(save_file)\n",
    "    \n",
    "    player.complete_current_battle()\n",
    "\n",
    "\n",
    "def agent_evaluation(player, agent, nb_episodes, filename):\n",
    "    '''\n",
    "    Function for testing the agent and saving the model history.\n",
    "    '''\n",
    "    \n",
    "    # Reset battle statistics\n",
    "    player.reset_battles()\n",
    "    model = agent.test(player, nb_episodes=nb_episodes, visualize=False, verbose=False)\n",
    "\n",
    "    # save model history to csv\n",
    "    save_file = f\"{filename}_testlog_{nb_episodes}eps.csv\"\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Saving model history as {save_file}\")\n",
    "    print(\"===============================================\")\n",
    "    pd.DataFrame(model.history).to_csv(save_file)\n",
    "    \n",
    "    print(\n",
    "          \"Model Evaluation: %d victories out of %d episodes\"\n",
    "          % (player.n_won_battles, nb_episodes)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Frozen RL Model and Load a Pretrained Model\n",
    "\n",
    "In the cell below, we define the appropriate Keras RL object for our preferred frozen model to use for the FrozenRLPlayer class. \n",
    "\n",
    "The loaded model should be one that was pretrained beforehand. \n",
    "\n",
    "To pretrain a RL model using the notebook, simply run the notebook while setting `FROZEN_MODEL_PRESENT` to `FALSE`. Replace the loaded model directories with the appropriate path to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output dimension\n",
    "N_ACTIONS = 18\n",
    "\n",
    "########################## Trained RL Model variables ##########################\n",
    "\n",
    "if FROZEN_MODEL_PRESENT:\n",
    "    \n",
    "    if FROZEN_MODEL_NAME == 'CEM':\n",
    "        \n",
    "        ### CHANGE THE LOAD MODEL DIRECTORY ACCORDING TO LOCAL SETUP ###\n",
    "        loaded_model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/modelmax_20000')\n",
    "        \n",
    "        memory = EpisodeParameterMemory(limit=10000, window_length=1)\n",
    "        trained_agent = CEMAgent(\n",
    "                        model=loaded_model, \n",
    "                        nb_actions=N_ACTIONS, \n",
    "                        memory=memory,\n",
    "                        batch_size=50, \n",
    "                        nb_steps_warmup=NB_STEPS_WARMUP, \n",
    "                        train_interval=50, \n",
    "                        elite_frac=0.05, \n",
    "                        noise_ampl=0\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    if FROZEN_MODEL_NAME == 'DQN':\n",
    "        \n",
    "        ### CHANGE THE LOAD MODEL DIRECTORY ACCORDING TO LOCAL SETUP ###\n",
    "        loaded_model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/src/DQNAgent/modeldqn_20000')\n",
    "        \n",
    "        # set memory for DQN Agent\n",
    "        memory = SequentialMemory(limit=10000, window_length=1)\n",
    "        \n",
    "        # Simple epsilon greedy policy for DQN Agent\n",
    "        policy = LinearAnnealedPolicy(\n",
    "            EpsGreedyQPolicy(),\n",
    "            attr=\"eps\",\n",
    "            value_max=1.0,\n",
    "            value_min=0.05,\n",
    "            value_test=0,\n",
    "            nb_steps=10000,\n",
    "        )\n",
    "\n",
    "        # load saved model into DQNAgent class\n",
    "        trained_agent = DQNAgent(\n",
    "                model=loaded_model,\n",
    "                nb_actions=N_ACTIONS,\n",
    "                policy=policy,\n",
    "                memory=memory,\n",
    "                nb_steps_warmup=NB_STEPS_WARMUP,\n",
    "                gamma=0.5,\n",
    "                target_model_update=1,\n",
    "                delta_clip=0.01,\n",
    "                enable_double_dqn=True,\n",
    "            )\n",
    "\n",
    "    elif FROZEN_MODEL_NAME == 'SARSA':\n",
    "        \n",
    "        ### CHANGE THE LOAD MODEL DIRECTORY ACCORDING TO LOCAL SETUP ###\n",
    "        loaded_model = tf.keras.models.load_model('/Users/nicarinanan/Desktop/poke-env/src/SARSAAgent/model_20000')\n",
    "        \n",
    "        memory = SequentialMemory(limit=10000, window_length=1)\n",
    "        \n",
    "        #Simple epsilon greedy policy\n",
    "        policy = LinearAnnealedPolicy(\n",
    "                            EpsGreedyQPolicy(),\n",
    "                            attr=\"eps\",\n",
    "                            value_max=1.0,\n",
    "                            value_min=0.05,\n",
    "                            value_test=0,\n",
    "                            nb_steps=10000,\n",
    "                        )\n",
    "        \n",
    "        trained_agent = SARSAAgent(\n",
    "                            model=loaded_model,          \n",
    "                            nb_actions=N_ACTIONS,          \n",
    "                            nb_steps_warmup=1000,         \n",
    "                            policy=policy\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Player Classes and Setting up RL Networks\n",
    "\n",
    "We are performing **deep** reinforcement learning, so we first need to define a Keras network for the training process. \n",
    "\n",
    "We define the network differently depending on whether we are using CEM, DQN or SARSA. \n",
    "\n",
    "In the cell below, we first instantiate the four different types of players that we use. \n",
    "\n",
    "Then, we define the neural network structures. For DQN & SARSA, we use identical structures and a linear activation, whilst we use a softmax activation for CEM. This makes sense because CEM automatically predicts the policy, whilst DQN and SARSA predict the q-values which are then converted into a policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent that we use for training or testing\n",
    "env_player = SimpleRLPlayer(\n",
    "    player_configuration=PlayerConfiguration(\"satunicarina\", None),\n",
    "    battle_format=\"gen7randombattle\",\n",
    "    server_configuration=LocalhostServerConfiguration,\n",
    ")\n",
    "\n",
    "\n",
    "random_opponent = RandomPlayer(\n",
    "    player_configuration=PlayerConfiguration(\"duanicarina\", None),\n",
    "    battle_format=\"gen7randombattle\",\n",
    "    server_configuration=LocalhostServerConfiguration,\n",
    ")\n",
    "\n",
    "max_opponent = MaxDamagePlayer(\n",
    "    player_configuration=PlayerConfiguration(\"tiganicarina\", None),\n",
    "    battle_format=\"gen7randombattle\",\n",
    "    server_configuration=LocalhostServerConfiguration,\n",
    ")\n",
    "\n",
    "if FROZEN_MODEL_PRESENT:\n",
    "    rl_opponent = FrozenRLPlayer(\n",
    "        player_configuration=PlayerConfiguration(\"empatnicarina\", None), \n",
    "        battle_format=\"gen7randombattle\", \n",
    "        server_configuration=LocalhostServerConfiguration,         \n",
    "        trained_rl_model=trained_agent,\n",
    "        model_name = FROZEN_MODEL_NAME,)\n",
    "\n",
    "\n",
    "if model_type == 'CEM':\n",
    "    \n",
    "    # Output dimension\n",
    "    memory = EpisodeParameterMemory(limit=10000, window_length=1)\n",
    "    \n",
    "    # deep network\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, 10)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(N_ACTIONS))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Simple epsilon greedy\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.05,\n",
    "        value_test=0,\n",
    "        nb_steps=10000,\n",
    "    )\n",
    "\n",
    "    # Defining our agent\n",
    "    agent = CEMAgent(model=model, nb_actions=N_ACTIONS, memory=memory,\n",
    "                   batch_size=50, nb_steps_warmup=1000, train_interval=50, elite_frac=0.05, noise_ampl=4)\n",
    "    agent.compile()\n",
    "\n",
    "    \n",
    "if model_type == 'DQN' or model_type == 'SARSA':\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"elu\", input_shape=(1, 10)))\n",
    "\n",
    "    # Our embedding have shape (1, 10), which affects our hidden layer\n",
    "    # dimension and output dimension\n",
    "    # Flattening resolve potential issues that would arise otherwise\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"elu\"))\n",
    "    model.add(Dense(N_ACTIONS, activation=\"linear\"))\n",
    "\n",
    "    memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "    # Simple epsilon greedy\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.05,\n",
    "        value_test=0,\n",
    "        nb_steps=10000,\n",
    "    )\n",
    "\n",
    "    # Defining our DQN\n",
    "    if model_type == 'DQN':\n",
    "        agent = DQNAgent(\n",
    "            model=model,\n",
    "            nb_actions=N_ACTIONS,\n",
    "            policy=policy,\n",
    "            memory=memory,\n",
    "            nb_steps_warmup=1000,\n",
    "            gamma=0.5,\n",
    "            target_model_update=1,\n",
    "            delta_clip=0.01,\n",
    "            enable_double_dqn=True,\n",
    "        )\n",
    "        \n",
    "    if model_type == 'SARSA':\n",
    "        agent = SARSAAgent(\n",
    "            model=model, \n",
    "            nb_actions=N_ACTIONS, \n",
    "            nb_steps_warmup=1000, \n",
    "            policy=policy\n",
    "        )\n",
    "\n",
    "    agent.compile(Adam(lr=0.00025), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Train!\n",
    "\n",
    "As a last step, we train the model against a pretrained RL opponent, then allow it to battle against a RandomOpponent (chooses a random move each time), a MaxDamageOpponent (chooses the move that delivers max damage each time), and RLOpponent (pretrained RL model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: 0.6976\n",
      "249 episodes - episode_reward: 27.992 [-42.590, 47.829] - loss: 0.012 - mae: 0.308 - mean_q: 0.515 - mean_eps: 0.478\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 132s 13ms/step - reward: 1.2118\n",
      "done, took 253.455 seconds\n",
      "===============================================\n",
      "Saving model history as FrozenRLPlayerDQN_trainlog_20000eps.csv\n",
      "===============================================\n",
      "INFO:tensorflow:Assets written to: modelpostdqn_20000/assets\n",
      "Results against random player:\n",
      "===============================================\n",
      "Saving model history as (FrozenRLPlayerDQN_20000)RandomPlayer_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 99 victories out of 100 episodes\n",
      "\n",
      "Results against max player:\n",
      "===============================================\n",
      "Saving model history as (FrozenRLPlayerDQN_20000)MaxPlayer_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 80 victories out of 100 episodes\n",
      "\n",
      "Results against frozen rl player:\n",
      "===============================================\n",
      "Saving model history as (FrozenRLPlayerDQN_20000)MaxPlayer_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 93 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "# Training the agent using a pretrained rl opponent (specifically DQN)\n",
    "env_player.play_against(\n",
    "    env_algorithm=agent_training,\n",
    "    opponent=rl_opponent,\n",
    "    env_algorithm_kwargs={\"agent\": agent, \"nb_steps\": NB_TRAINING_STEPS, \"filename\": TRAINING_OPPONENT},\n",
    ")\n",
    "\n",
    "# save the trained model (for setting up rl opponent later)\n",
    "model.save(\"model_postmaxcem_%d\" % NB_TRAINING_STEPS)\n",
    "\n",
    "# Evaluating the agent\n",
    "print(\"Results against random player:\")\n",
    "env_player.play_against(\n",
    "    env_algorithm=agent_evaluation,\n",
    "    opponent=random_opponent,\n",
    "    env_algorithm_kwargs={\"agent\": agent, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'Trained{TRAINING_OPPONENT}({NB_TRAINING_STEPS})vsRandomPlayer'},\n",
    ")\n",
    "\n",
    "print(\"\\nResults against max player:\")\n",
    "env_player.play_against(\n",
    "    env_algorithm=agent_evaluation,\n",
    "    opponent=max_opponent,\n",
    "    env_algorithm_kwargs={\"agent\": agent, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'Trained{TRAINING_OPPONENT}({NB_TRAINING_STEPS})vsMaxPlayer'},\n",
    ")\n",
    "\n",
    "if FROZEN_MODEL_PRESENT:\n",
    "    print(\"\\nResults against frozen rl player:\")\n",
    "    env_player.play_against(\n",
    "    env_algorithm=agent_evaluation,\n",
    "    opponent=rl_opponent,\n",
    "    env_algorithm_kwargs={\"agent\": agent, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'Trained{TRAINING_OPPONENT}({NB_TRAINING_STEPS})vsRLPlayer'},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting and Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# functions for creating the df and plots\n",
    "\n",
    "def create_df(csv_file):\n",
    "    '''\n",
    "    create df from csv file\n",
    "    '''\n",
    "    train = pd.read_csv(csv_file)\n",
    "    train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    return train\n",
    "\n",
    "def create_plots(train_log, train_opponent, plot_color='blue'):\n",
    "    '''\n",
    "    Create plots from data.\n",
    "    \n",
    "    train_log (DATAFRAME)\n",
    "        A dataframe created from create_df() function.\n",
    "        \n",
    "    train_opponent (STR)\n",
    "        String input that will be used in the plot titles. \n",
    "        Specifies the type of opponent.\n",
    "        \n",
    "    plot_color (STR)\n",
    "        Specifies the color of the plots.\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize=(16,4), dpi=120)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(train_log['nb_steps'], train_log['episode_reward'], color = plot_color, alpha = 0.7)\n",
    "    plt.title(f\"DQN Agent Trained Against {train_opponent}Player\", size=15)\n",
    "    plt.xlabel('Number of Steps')\n",
    "    plt.ylabel('Episode Reward')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(train_log['episode_reward'], color = plot_color, alpha = 0.7)\n",
    "    plt.title(f\"Reward Histogram of DQN Agent Trained Against {train_opponent} Player \\n Mean reward: {round(train_log['episode_reward'].mean(), 2)}\", size=15)\n",
    "    plt.xlabel('Episode Reward')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained DQN Agent: Agent trained against Pretrained DQN Opponent \n",
    "\n",
    "The plots are shown in the following order:\n",
    "\n",
    "- *Trained DQN Agent vs Random Opponent*\n",
    "- *Trained DQN Agent vs Max Opponent*\n",
    "- *Trained DQN Agent vs Pretrained DQN Opponent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_vs_random = create_df('TrainedPretrainedRLPlayer(20000)vsRandomPlayer_testlog_1000eps.csv')\n",
    "create_plots(rl_vs_random, 'DQN vs Random', 'SteelBlue')\n",
    "\n",
    "rl_vs_max = create_df('TrainedPretrainedRLPlayer(20000)vsMaxPlayer_testlog_1000eps.csv')\n",
    "create_plots(rl_vs_max, 'DQN vs Max', 'coral')\n",
    "\n",
    "rl_vs_rl = create_df('TrainedPRetrainedRLPlayer(20000)vsRLPlayer_testlog_1000eps.csv')\n",
    "create_plots(rl_vs_rl, 'DQN vs Pretrained DQN', 'orange')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
