{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/nicarinanan/Desktop/poke-env/krl/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -0.0156\n",
      "done, took 60.485 seconds\n",
      "===============================================\n",
      "Saving model history as FrozenRLPlayer_trainlog_10000eps.csv\n",
      "===============================================\n",
      "INFO:tensorflow:Assets written to: model_10000/assets\n",
      "Results against random player:\n",
      "===============================================\n",
      "Saving model history as (FrozenRLPlayer_10000)RandomPlayer_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 55 victories out of 100 episodes\n",
      "\n",
      "Results against max player:\n",
      "===============================================\n",
      "Saving model history as (FrozenRLPlayer_10000)MaxPlayer_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 13 victories out of 100 episodes\n",
      "\n",
      "Results against frozen rl player:\n",
      "===============================================\n",
      "Saving model history as (FrozenRLPlayer_10000)MaxPlayer_testlog_100eps.csv\n",
      "===============================================\n",
      "CEM Evaluation: 57 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.player.env_player import Gen7EnvSinglePlayer\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.frozen_rl_player import FrozenRLPlayer\n",
    "from poke_env.server_configuration import LocalhostServerConfiguration\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory, EpisodeParameterMemory\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# We define our RL player\n",
    "# It needs a state embedder and a reward computer, hence these two methods\n",
    "class SimpleRLPlayer(Gen7EnvSinglePlayer):\n",
    "    def embed_battle(self, battle):\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have not fainted in each team\n",
    "        remaining_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "        remaining_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        return np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [remaining_mon_team, remaining_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            battle, fainted_value=2, hp_value=1, victory_value=30\n",
    "        )\n",
    "\n",
    "\n",
    "class MaxDamagePlayer(RandomPlayer):\n",
    "    def choose_move(self, battle):\n",
    "        # If the player can attack, it will\n",
    "        if battle.available_moves:\n",
    "            # Finds the best move among available ones\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "\n",
    "        # If no attack is available, a random switch will be made\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n",
    "\n",
    "\n",
    "NB_TRAINING_STEPS = 10000\n",
    "NB_EVALUATION_EPISODES = 100\n",
    "\n",
    "# variable for naming .csv files.\n",
    "# Change this according to whether the training process was carried out against a random player or a max damage player\n",
    "TRAINING_OPPONENT = 'FrozenRLPlayer'\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# This is the function that will be used to train the dqn\n",
    "def dqn_training(player, dqn, nb_steps, filename):\n",
    "    model = dqn.fit(player, nb_steps=nb_steps)\n",
    "    # save model history to csv\n",
    "    save_file = f\"{filename}_trainlog_{nb_steps}eps.csv\"\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Saving model history as {save_file}\")\n",
    "    print(\"===============================================\")\n",
    "    pd.DataFrame(model.history).to_csv(save_file)\n",
    "    player.complete_current_battle()\n",
    "\n",
    "\n",
    "def dqn_evaluation(player, dqn, nb_episodes, filename):\n",
    "    # Reset battle statistics\n",
    "    player.reset_battles()\n",
    "    model = dqn.test(player, nb_episodes=nb_episodes, visualize=False, verbose=False)\n",
    "\n",
    "    # save model history to csv\n",
    "    save_file = f\"{filename}_testlog_{nb_episodes}eps.csv\"\n",
    "    print(\"===============================================\")\n",
    "    print(f\"Saving model history as {save_file}\")\n",
    "    print(\"===============================================\")\n",
    "    pd.DataFrame(model.history).to_csv(save_file)\n",
    "    \n",
    "    print(\n",
    "          \"CEM Evaluation: %d victories out of %d episodes\"\n",
    "          % (player.n_won_battles, nb_episodes)\n",
    "          )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_player = SimpleRLPlayer(\n",
    "        player_configuration=PlayerConfiguration(\"satunicarina\", None),\n",
    "        battle_format=\"gen7randombattle\",\n",
    "        server_configuration=LocalhostServerConfiguration,\n",
    "    )\n",
    "\n",
    "    opponent = RandomPlayer(\n",
    "        player_configuration=PlayerConfiguration(\"duanicarina\", None),\n",
    "        battle_format=\"gen7randombattle\",\n",
    "        server_configuration=LocalhostServerConfiguration,\n",
    "    )\n",
    "\n",
    "    second_opponent = MaxDamagePlayer(\n",
    "        player_configuration=PlayerConfiguration(\"tiganicarina\", None),\n",
    "        battle_format=\"gen7randombattle\",\n",
    "        server_configuration=LocalhostServerConfiguration,\n",
    "    )\n",
    "    \n",
    "    third_opponent = FrozenRLPlayer(\n",
    "                                    player_configuration=PlayerConfiguration(\"empatnicarina\", None),\n",
    "                                    battle_format=\"gen7randombattle\",\n",
    "                                    server_configuration=LocalhostServerConfiguration\n",
    "                                    )\n",
    "\n",
    "#    third_opponent = pickle.load(\"previousRLPlayer\")\n",
    "\n",
    "    # Output dimension\n",
    "    n_action = len(env_player.action_space)\n",
    "\n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(128, activation=\"elu\", input_shape=(1, 10)))\n",
    "#    model.add(Flatten())\n",
    "#    model.add(Dense(n_action))\n",
    "#    model.add(Activation('softmax'))\n",
    "    memory = EpisodeParameterMemory(limit=10000, window_length=1)\n",
    "# Option 2: deep network\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, 10)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(n_action))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Ssimple epsilon greedy\n",
    "    policy = LinearAnnealedPolicy(\n",
    "        EpsGreedyQPolicy(),\n",
    "        attr=\"eps\",\n",
    "        value_max=1.0,\n",
    "        value_min=0.05,\n",
    "        value_test=0,\n",
    "        nb_steps=10000,\n",
    "    )\n",
    "\n",
    "    # Defining our DQN\n",
    "    dqn = CEMAgent(model=model, nb_actions=n_action, memory=memory,\n",
    "                   batch_size=50, nb_steps_warmup=1000, train_interval=50, elite_frac=0.05, noise_ampl=4)\n",
    "    \n",
    "\n",
    "    dqn.compile()\n",
    "\n",
    "    # Training\n",
    "    env_player.play_against(\n",
    "        env_algorithm=dqn_training,\n",
    "        opponent=third_opponent,\n",
    "                            env_algorithm_kwargs={\"dqn\": dqn, \"nb_steps\": NB_TRAINING_STEPS, \"filename\": TRAINING_OPPONENT},\n",
    "    )\n",
    "    model.save(\"model_%d\" % NB_TRAINING_STEPS)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"Results against random player:\")\n",
    "    env_player.play_against(\n",
    "        env_algorithm=dqn_evaluation,\n",
    "        opponent=opponent,\n",
    "        env_algorithm_kwargs={\"dqn\": dqn, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'({TRAINING_OPPONENT}_{NB_TRAINING_STEPS})RandomPlayer'},\n",
    "    )\n",
    "\n",
    "    print(\"\\nResults against max player:\")\n",
    "    env_player.play_against(\n",
    "        env_algorithm=dqn_evaluation,\n",
    "        opponent=second_opponent,\n",
    "        env_algorithm_kwargs={\"dqn\": dqn, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'({TRAINING_OPPONENT}_{NB_TRAINING_STEPS})MaxPlayer'},\n",
    "    )\n",
    "\n",
    "    print(\"\\nResults against frozen rl player:\")\n",
    "    env_player.play_against(\n",
    "                            env_algorithm=dqn_evaluation,\n",
    "                            opponent=third_opponent,\n",
    "                            env_algorithm_kwargs={\"dqn\": dqn, \"nb_episodes\": NB_EVALUATION_EPISODES, \"filename\": f'({TRAINING_OPPONENT}_{NB_TRAINING_STEPS})MaxPlayer'},\n",
    "                            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krl-env",
   "language": "python",
   "name": "krl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
